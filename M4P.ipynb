{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6518d1ad-e9ce-4f4c-ae69-4e4216c23342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从MP3, MP4, M4P 文件直接转化成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9387d4-ca95-41bb-a432-d4e7ab9efa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install SpeechRecognition\n",
    "# pip install pydub\n",
    "# !ffmpeg -version\n",
    "# pip install transformers torch\n",
    "# pip install python-docx\n",
    "# !pip install --upgrade pip setuptools wheel\n",
    "# !pip install cmake\n",
    "# !pip uninstall -y sentencepiece\n",
    "# !pip install sentencepiece\n",
    "# !pip install sentencepiece==0.1.96\n",
    "# !D:/Users/User/anaconda3/python.exe -m pip install --upgrade pip setuptools wheel\n",
    "# pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de379c9a-241c-4fa3-b38e-a87910edb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9c0bd4-9033-4de4-a3ed-e2d9664962b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "音频已转换为 WAV 格式\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: 将 M4A 文件转换为 WAV 格式\n",
    "audio = AudioSegment.from_file(\"D:/Recording.m4a\", format=\"m4a\")  # 请将路径调整为你的文件路径\n",
    "audio.export(\"D:/converted.wav\", format=\"wav\")\n",
    "print(\"音频已转换为 WAV 格式\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097ca3b5-232b-49e0-8331-4f2d6e02d7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语音识别结果： 啊好把这一部分的内容总结一下首先呢是一个人他有一个认知体系那么这个人认知体系他肯定会基于这个人的特色然后有一些特定的那个触发点那么这个出发点来自于这个人跟其他的事物进行一个交互那这个比如说他可能跟另一个人进行一段对话或者是看了一部电影看了一本书甚至死是听了一部而听了一个演讲然后他都会产生一些兴趣点或者是触发点但他觉得眼前一亮想要探索更多那么这些出发点之间的有一些可能是抢连结的有一些可能是弱连接的但是现在可能就在一段时间之后就就他就回忆不起来了但是强连接的呢他因为大脑比如说基于这个出发点他有很多在接受到这个出发点的时候他有很多联想比如说当然他是主动的进行联想或者是被动了一下然后呢这样的话就会让这个出发点呢就会牢牢的就是说被保留在了他的知识体系列在他的知识体系在这个范围内就不断的回扩大然后呢因为就是因为这个强连接那么这个新的知识听了这个出发点也变成了他只是体系的一个部分然后呢也在未来的使用当中更容易被调去但是但是那些弱点可能未来在未来的使用中就被遗忘就不会被调去了那么然后还有一个方法它能帮助我们去就是主动的去增强这些弱连结那么就是人为的进行主动的去教别人或者是去输出比如说路上一个视频写一段话当然在这个过程中呢他就会不断的主动的去掉去那么就会加深这个内容跟原有之体系的一些交互然后进行连接当然这部分如果要持续的话他就需要一个反馈那个反馈可能就是来自于比如说读者听众任何一个外部的环境那么这里就好像有一次的一些现象就是说我们在接收到好的反馈的时候我们的情感恼他会突然就会被激发他就会很快然后就是本人的一部分那么这个人做这个事情就非常有个动力我们想想一下你还来原来在在学校里面嗯跟就收到老师的表扬之类的或者是家长的这个认可那么就会特别有动力去把这个事情哦再见这里要干好但是呢如果遇到了负面的反馈的话那就要启动我们的理性脑那我们通过一种合理的解释不要掉入到这个方面富贵的这个循环里面这个负面的评价里面因为如果到了父母评价也那么人如果任由这个情绪控制的话那个情绪就会被外界影响就会也变得负面但其实这期的并不重要因为总会有一部分人他会因为我们的一些内容出出然后呢得到一些收益但是我们签订要讨论的是我们是不是这一部分内容它是经过我们精心打磨就是清新打造的就像书中描述的一样他说我想把我们的作品当成一个孩子一样带出去打扮的漂漂亮亮我觉得这个解释平衡的形象生动也让我觉得我对于做这个视频博主的这个事情有了一些深入的一些思考和理解我觉得这可以是一个很好的一个方向\n",
      "识别文本已保存到 D:/output.txt 文件中\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 使用 Google 语音识别，将音频转为文本\n",
    "recognizer = sr.Recognizer()\n",
    "with sr.AudioFile(\"D:/converted.wav\") as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "    text = recognizer.recognize_google(audio_data, language=\"zh-CN\")\n",
    "    print(\"语音识别结果：\", text)\n",
    "# 将识别的文本保存到 .txt 文件\n",
    "with open(\"D:/output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text)\n",
    "print(\"识别文本已保存到 D:/output.txt 文件中\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f9518-fb8f-4ca7-92ef-ab6d284753f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a244216-c2fd-424c-b6b0-4a7ea47df0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93072649-1b9d-4577-b057-8e5cbb50f9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daef0cfe89384ff3a065584b21d0c40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677bddf613f3484393120de1f1eef411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de29874598904a3092995c377c933714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11157ba7d0124057a1cc272d2320ad46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389a677642234a16b159a25476e3a82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c89850b08446b9a6b8effa202b9012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b364f8d5ccb34e6a956ba830b9507934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "优化后的文本已保存到 optimized_text.txt 文件中。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 加载Flan-T5 large模型和分词器\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 读取外部文件中的文本\n",
    "with open(\"D:/output.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "# 对输入文本进行tokenize并生成输出\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100, num_beams=5, early_stopping=True)\n",
    "\n",
    "# 将生成的token解码为文本\n",
    "optimized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 将优化后的文本写入到新的输出文件\n",
    "with open(\"D:/optimized_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(optimized_text)\n",
    "\n",
    "print(\"优化后的文本已保存到 optimized_text.txt 文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8bef92e-f3b9-471b-a0b3-a9256bc11af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0ba46df79c453f83aa74563dbe8406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--utrobinmv--t5_summary_en_ru_zh_base_2048. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3daa28ec3ed4236a988ff0dd1064ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c4f353b66b41cf8cc876fd03979fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c134098f1e834ac29946fb5238d07520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79f6fb001014ec5a52fd861caaced9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/809 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835c79c3f6224ac99c4addef03ec25fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d890a6a53a4dddbe96029ab05fa955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割后的句子: ['', '啊好把这一部分的内容总结一下', '首先呢是一个人他有一个认知体系', '那么这个人认知体系他肯定会基于这个人的特色', '然后有一些特定的那个触发点', '那么这个出发点来自于这个人跟其他的事物进行一个交互那这个比如说他可能跟另一个人进行一段对话或者是看了一部电影看了一本书甚至死是听了一部而听了一个演讲', '然后他都会产生一些兴趣点或者是触发点但他觉得眼前一亮想要探索更多', '那么这些出发点之间的有一些可能是抢连结的有一些可能是弱连接的但是现在可能就在一段时间', '之后就就他就回忆不起来了但是强连接的呢他因为大脑比如说基于这个出发点他有很多在接受到这个出发点的时候他有很多联想比如说当然他是主动的进行联想或者是被动了一下', '然后呢这样的话就会让这个出发点呢就会牢牢的就是说被保留在了他的知识体系列在他的知识体系在这个范围内就不断的回扩大', '然后呢因为就是因为这个强连接', '那么这个新的知识听了这个出发点也变成了他只是体系的一个部分', '然后呢也在未来的使用当中更容易被调去但是但是那些弱点可能未来在未来的使用中就被遗忘就不会被调去了', '那么', '然后还有一个方法它能帮助我们去就是主动的去增强这些弱连结', '那么就是人为的进行主动的去教别人或者是去输出比如说路上一个视频写一段话当然在这个过程中呢他就会不断的主动的去掉去', '那么就会加深这个内容跟原有之体系的一些交互', '然后进行连接当然这部分如果要持续的话他就需要一个反馈那个反馈可能就是来自于比如说读者听众任何一个外部的环境', '那么这里就好像有一次的一些现象就是说我们在接收到好的反馈的时候我们的情感恼他会突然就会被激发他就会很快', '然后就是本人的一部分', '那么这个人做这个事情就非常有个动力我们想想一下你还来原来在在学校里面', '嗯跟就收到老师的表扬之类的或者是家长的这个认可', '那么就会特别有动力去把这个事情哦再见这里要干好但是呢如果遇到了负面的反馈的话那就要启动我们的理性脑那我们通过一种合理的解释不要掉入到这个方面富贵的这个循环里面这个负面的评价里面因为如果到了父母评价也', '那么人如果任由这个情绪控制的话那个情绪就会被外界影响就会也变得负面但其实这期的并不重要因为总会有一部分人他会因为我们的一些内容出出', '然后呢得到一些收益但是我们签订要讨论的是我们是不是这一部分内容它是经过我们精心打磨就是清新打造的就像书中描述的一样他说', '我想把我们的作品当成一个孩子一样带出去打扮的漂漂亮亮', '我觉得这个解释平衡的形象生动也让', '我觉得我对于做这个视频博主的这个事情有了一些深入的一些思考和理解', '我觉得这可以是一个很好的一个方向']\n",
      "优化后的文本:\n",
      " 回顾一下这一部分的内容。 啊,把这一点总结一下。. 首先,是人他有一个认知体系。首先是他有意识体系,然后是一个人。. 这个人认知体系他肯定会基于这个人的特色。那么这个人认识体系他会基于他的特色:. 周二(10月14日)是英国人节。 有些有点儿了。. 那么这个出发点来自于这个人跟其他的事物进行一个交互,比如说他可能跟另一个人进行一段对话或者是看了一部电影看了一本书甚至死是听了一场演讲。. 2012年11月14日, 1979年9月30日早晨,他终于开始幻想。. 距离这些出发点之间的距离可能还有一段时间,但是现在可能已经一段时间了。. 后来就他就回忆不起来了但是强连接的呢?他因为大脑比如说基于这个出发点他有很多在接受到这个方向点的时候,他还有很多联想或者被动了一下。. 写在他的知识体系列在他的知识体系在这个范围内就不断的回扩大了。. 回想一下,这是不是有了这个强联吗?. 一个新的知识听了这个出发点,也变成了他只是体系的一部分。这个新知识听到了这一步。. 在未来的使用中更容易被调去,但那些弱点可能未来未来的如何使用中就被遗忘就不会被修正。. 那么,你是不是觉得你得了吗?. 一个方法可以帮助我们去的是主动的,改善弱连结。. 人为的进行主动的去教别人或者是去输出,比如说路上一个视频写一段话当然在这个过程中,他就就会不断的主动去掉去。. 这段文本将加深内容与原有的体系的一些交互。同时,文章也提到了将这一内容添加到了原创的版本。. 进行连接时,如果要持续,他就需要一个反馈这个反馈的人。. 我们在接收到好的反馈时,我们的情感刺激会突然被激发他就会很快。. 1994年11月14日,在荷兰首都吉隆坡,举行了一场音乐会。. 这个人做这个事情就非常有个动力,我们想想你还去原来在学校里面。. 嗯跟就收到老师的表扬之类的或者是家长的认可。学校的老师们对此表示欢迎。. 如果遇到了负面的反馈的话,就要启动我们的理性脑那我们通过一种合理的解释不要掉入到这个方面富贵的循环里面这个负面评价里面。. 那么人如果任由这个情绪控制的话,那个情绪就会被外界影响就会也变得负面,但其实这期的并不重要因为总会有一部分人他会因为我们的一些内容出出。. 我们签要讨论的我们是不是这一部分内容是经过我们精心打磨的清新打造的如同书中描述的。. 我想把我们的作品当成一个孩子一样带出去打扮的样子。. 我觉得这个解释平衡的形象生动也让这一点产生影响。. 想想做这个视频博主的感受。我对制作此视频感到担忧。. 我觉得这可以是一个很好的一个方向。在这一点上,我肯定会对这一点感到满意。.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # 加载Flan-T5 large模型和分词器\n",
    "# model_name = \"uer/t5-v1_1-base-chinese-cluecorpussmall\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"utrobinmv/t5_summary_en_ru_zh_base_2048\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 从文件读取无标点的输入文本\n",
    "with open(\"D:/output - Copy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "# 自动分割文本成更短的句子\n",
    "# 假设用“那么”、“然后”等词作为自然的分隔点插入句号\n",
    "split_text = re.sub(r\"(那么|然后|首先|所以|因此|之后|啊|嗯|我觉得|我想|我认为)\", r\". \\1\", input_text)\n",
    "split_sentences = split_text.split(\". \")  # 分割成短句子\n",
    "print(\"分割后的句子:\", split_sentences)\n",
    "\n",
    "# 对每个短句子生成优化后的文本\n",
    "optimized_sentences = []\n",
    "for sentence in split_sentences:\n",
    "    if sentence.strip():  # 忽略空白句子\n",
    "        # 移除 token_type_ids\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "        \n",
    "        # 调用生成函数\n",
    "        outputs = model.generate(**inputs, max_length=300, num_beams=6, early_stopping=True)\n",
    "        optimized_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        optimized_sentences.append(optimized_sentence)\n",
    "\n",
    "# 将优化后的文本写入到输出文件\n",
    "with open(\"optimized_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\". \".join(optimized_sentences) + \".\")\n",
    "\n",
    "# 打印优化后的文本\n",
    "optimized_text = \". \".join(optimized_sentences) + \".\"\n",
    "print(\"优化后的文本:\\n\", optimized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e711d-eaa2-4541-8d76-6f0507f35000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a244549-e79c-4af6-b5bd-d0072cedaaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed878e1ccf084509a06bebc78940efb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--csebuetnlp--mT5_m2o_chinese_simplified_crossSum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce96a85cc6084b479a9698f8b04e9026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c512195cfcd42a9b9cde597eae51e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08667a1d6f54ed3b14a4c572e2afa76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2b8f93715a4083a03baeb30b1cf174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割后的句子: ['', '啊好把这一部分的内容总结一下', '首先呢是一个人他有一个认知体系', '那么这个人认知体系他肯定会基于这个人的特色', '然后有一些特定的那个触发点', '那么这个出发点来自于这个人跟其他的事物进行一个交互那这个比如说他可能跟另一个人进行一段对话或者是看了一部电影看了一本书甚至死是听了一部而听了一个演讲', '然后他都会产生一些兴趣点或者是触发点但他觉得眼前一亮想要探索更多', '那么这些出发点之间的有一些可能是抢连结的有一些可能是弱连接的但是现在可能就在一段时间', '之后就就他就回忆不起来了但是强连接的呢他因为大脑比如说基于这个出发点他有很多在接受到这个出发点的时候他有很多联想比如说当然他是主动的进行联想或者是被动了一下', '然后呢这样的话就会让这个出发点呢就会牢牢的就是说被保留在了他的知识体系列在他的知识体系在这个范围内就不断的回扩大', '然后呢因为就是因为这个强连接', '那么这个新的知识听了这个出发点也变成了他只是体系的一个部分', '然后呢也在未来的使用当中更容易被调去但是但是那些弱点可能未来在未来的使用中就被遗忘就不会被调去了', '那么', '然后还有一个方法它能帮助我们去就是主动的去增强这些弱连结', '那么就是人为的进行主动的去教别人或者是去输出比如说路上一个视频写一段话当然在这个过程中呢他就会不断的主动的去掉去', '那么就会加深这个内容跟原有之体系的一些交互', '然后进行连接当然这部分如果要持续的话他就需要一个反馈那个反馈可能就是来自于比如说读者听众任何一个外部的环境', '那么这里就好像有一次的一些现象就是说我们在接收到好的反馈的时候我们的情感恼他会突然就会被激发他就会很快', '然后就是本人的一部分', '那么这个人做这个事情就非常有个动力我们想想一下你还来原来在在学校里面', '嗯跟就收到老师的表扬之类的或者是家长的这个认可', '那么就会特别有动力去把这个事情哦再见这里要干好但是呢如果遇到了负面的反馈的话那就要启动我们的理性脑那我们通过一种合理的解释不要掉入到这个方面富贵的这个循环里面这个负面的评价里面因为如果到了父母评价也', '那么人如果任由这个情绪控制的话那个情绪就会被外界影响就会也变得负面但其实这期的并不重要因为总会有一部分人他会因为我们的一些内容出出', '然后呢得到一些收益但是我们签订要讨论的是我们是不是这一部分内容它是经过我们精心打磨就是清新打造的就像书中描述的一样他说', '我想把我们的作品当成一个孩子一样带出去打扮的漂漂亮亮', '我觉得这个解释平衡的形象生动也让', '我觉得我对于做这个视频博主的这个事情有了一些深入的一些思考和理解', '我觉得这可以是一个很好的一个方向']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047e7080e28d4884a9798275d377ec4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "优化后的文本:\n",
      " <extra_id_59> 香港大学校长黄之锋最近写了一篇 的 特写。. <extra_id_59> 在中美贸易战的紧张局势中,还存在另一个争议—合成的人。. <extra_id_59> 人类的认知这种看法究竟是什么?. <extra_id_59> 新冠病毒在全球肆虐至今,已经造成超过四千万人感染、超过一百万人死亡。. <extra_id_59> 你有没有遇到过类似情况:向朋友抱怨和伴侣之间的问题,但他们却觉得没什么好担心的?又或者你觉得朋友的新对象完全不适合,但他俩感情却越来越好?. <extra_id_59> 在爱丁堡大学校园里,坐落着一位名叫丹(Dan)的艺术家。. <extra_id_59> 在过去的25年里,全球两个超级大国之间出现了一道桥梁。. <extra_id_59> 首先来到了小时候的雷蒙德还是一个小孩了。原来的他因为在起初的记忆中出现了一个原来的强连接。. <extra_id_59> 以阅读一下耶路撒冷的创始人兼作家马克·J·塞弗尔(Mark J Seifer)的《知识分子》是怎么回事?. <extra_id_59> 在美国华盛顿的防空识别站上,人们发现了来自北美和东亚的防空识别系统。. <extra_id_59> 上世纪80年代初,科学界就的一个概念提出了一个新的概念。. <extra_id_59> 许多人为了在未来的电脑上能发现的一些尖端和不一致的switching功能而遭到破坏。. <extra_id_59> BBC 英伦网 报道称, 香港铜锣湾书店星期一(12月27日)新楼声称有人正在破坏他们的牙齿。. <extra_id_59> 新冠病毒疫情不是人类第一次面对重大危机。. <extra_id_59> 跟在网上打交道的时候,你是不是会遇到过寻求帮助?. <extra_id_59> 在中美贸易战的紧张局势中,还存在一些误解。. <extra_id_59> 视频共享应用程序提供联络是否存在漏洞?. <extra_id_59> 有一次,詹姆斯(Boo James)女士搭乘公共汽车,看到一名陌生女子向她招手。当时,她没有多想,事后才知道其实那是她的亲妈。. <extra_id_59> 2016年9月, 哈里王子和妻子梅根宣布,他们正在日本访问。. <extra_id_59> 新学期开学在即,有很多在英格兰和威尔士的学子第一次踏进大学校园。. <extra_id_59> 家常便饭的老师,对学校的反应不一。. <extra_id_59> 我相信你肯定同意我的这个看法:人们通常会认为你认为自己有多聪明。. <extra_id_59> 你是否压力过大,不堪重负,任务繁多?. <extra_id_59> 经过了大约两年的调查和研究,我们决定把《哈利·波特》杂志的一些视频分享给大家。. <extra_id_59> 2016年发生许多大事件,以下是各地通讯社摄影记者拍到最好看的照片。. <extra_id_59> 毛泽东的新年致辞被外界赞赏。. <extra_id_59> BBC中文网采访了一位视频博主,请他谈谈对这个视频分享网站的看法。. <extra_id_59> 新冠病毒给全球经济带来重创。.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"csebuetnlp/mT5_m2o_chinese_simplified_crossSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 从文件读取无标点的输入文本\n",
    "with open(\"D:/output - Copy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "# 自动分割文本成更短的句子\n",
    "# 假设用“那么”、“然后”等词作为自然的分隔点插入句号\n",
    "split_text = re.sub(r\"(那么|然后|首先|所以|因此|之后|啊|嗯|我觉得|我想|我认为)\", r\". \\1\", input_text)\n",
    "split_sentences = split_text.split(\". \")  # 分割成短句子\n",
    "print(\"分割后的句子:\", split_sentences)\n",
    "\n",
    "# 对每个短句子生成优化后的文本\n",
    "optimized_sentences = []\n",
    "for sentence in split_sentences:\n",
    "    if sentence.strip():  # 忽略空白句子\n",
    "        # 移除 token_type_ids\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "        \n",
    "        # 调用生成函数\n",
    "        outputs = model.generate(**inputs, max_length=300, num_beams=6, early_stopping=True)\n",
    "        optimized_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        optimized_sentences.append(optimized_sentence)\n",
    "\n",
    "# 将优化后的文本写入到输出文件\n",
    "with open(\"optimized_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\". \".join(optimized_sentences) + \".\")\n",
    "\n",
    "# 打印优化后的文本\n",
    "optimized_text = \". \".join(optimized_sentences) + \".\"\n",
    "print(\"优化后的文本:\\n\", optimized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa695c7e-6a03-44c9-9c12-a532be55047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割后的句子: ['', '啊好把这一部分的内容总结一下', '首先呢是一个人他有一个认知体系', '那么这个人认知体系他肯定会基于这个人的特色', '然后有一些特定的那个触发点', '那么这个出发点来自于这个人跟其他的事物进行一个交互那这个比如说他可能跟另一个人进行一段对话或者是看了一部电影看了一本书甚至死是听了一部而听了一个演讲', '然后他都会产生一些兴趣点或者是触发点但他觉得眼前一亮想要探索更多', '那么这些出发点之间的有一些可能是抢连结的有一些可能是弱连接的但是现在可能就在一段时间', '之后就就他就回忆不起来了但是强连接的呢他因为大脑比如说基于这个出发点他有很多在接受到这个出发点的时候他有很多联想比如说当然他是主动的进行联想或者是被动了一下', '然后呢这样的话就会让这个出发点呢就会牢牢的就是说被保留在了他的知识体系列在他的知识体系在这个范围内就不断的回扩大', '然后呢因为就是因为这个强连接', '那么这个新的知识听了这个出发点也变成了他只是体系的一个部分', '然后呢也在未来的使用当中更容易被调去但是但是那些弱点可能未来在未来的使用中就被遗忘就不会被调去了', '那么', '然后还有一个方法它能帮助我们去就是主动的去增强这些弱连结', '那么就是人为的进行主动的去教别人或者是去输出比如说路上一个视频写一段话当然在这个过程中呢他就会不断的主动的去掉去', '那么就会加深这个内容跟原有之体系的一些交互', '然后进行连接当然这部分如果要持续的话他就需要一个反馈那个反馈可能就是来自于比如说读者听众任何一个外部的环境', '那么这里就好像有一次的一些现象就是说我们在接收到好的反馈的时候我们的情感恼他会突然就会被激发他就会很快', '然后就是本人的一部分', '那么这个人做这个事情就非常有个动力我们想想一下你还来原来在在学校里面', '嗯跟就收到老师的表扬之类的或者是家长的这个认可', '那么就会特别有动力去把这个事情哦再见这里要干好但是呢如果遇到了负面的反馈的话那就要启动我们的理性脑那我们通过一种合理的解释不要掉入到这个方面富贵的这个循环里面这个负面的评价里面因为如果到了父母评价也', '那么人如果任由这个情绪控制的话那个情绪就会被外界影响就会也变得负面但其实这期的并不重要因为总会有一部分人他会因为我们的一些内容出出', '然后呢得到一些收益但是我们签订要讨论的是我们是不是这一部分内容它是经过我们精心打磨就是清新打造的就像书中描述的一样他说', '我想把我们的作品当成一个孩子一样带出去打扮的漂漂亮亮', '我觉得这个解释平衡的形象生动也让', '我觉得我对于做这个视频博主的这个事情有了一些深入的一些思考和理解', '我觉得这可以是一个很好的一个方向']\n",
      "生成的最终摘要:\n",
      " <extra_id_59> 香港大学校长黄之锋最近写了一篇 的 特写。 <extra_id_59> 在中美贸易战的紧张局势中,还存在另一个争议—合成的人。 <extra_id_59> 人类的认知这种看法究竟是什么? <extra_id_59> 新冠病毒在全球肆虐至今,已经造成超过四千万人感染、超过一百万人死亡。 <extra_id_59> 你有没有遇到过类似情况:向朋友抱怨和伴侣之间的问题,但他们却觉得没什么好担心的?又或者你觉得朋友的新对象完全不适合,但他俩感情却越来越好? <extra_id_59> 在爱丁堡一隅,坐落着一位名叫阿纳斯塔的神秘博士。 <extra_id_59> 在过去的25年里,全球两个超级大国之间出现了一道桥梁。 <extra_id_59> 首先来到了小时候的雷蒙德还是一个小孩了。原来的他因为在起初的记忆中出现了一个强连接的地方。 <extra_id_59> 以阅读一下耶路撒冷的创始人兼作家马克·J·塞弗尔(Mark J Seifer)的《哈利·波特》是一位很有意思的人。 <extra_id_59> 你可能听说过在加州和加拿大之间存在网络信号吗? <extra_id_59> 从2000年开始,英国就开始了有关知识的实验。 <extra_id_59> 许多人为了在未来的电脑上能发现的一些尖端和不一致的加密数字装置而遭到破坏。 <extra_id_59> BBC 英伦网 在此访问中分享了来自法国诺贝尔和平奖得主的经验。 <extra_id_59> 新冠病毒疫情不是人类第一次面对重大危机。 <extra_id_59> 你是不是因为在网上遇到过寻求帮助? <extra_id_59> 在中美贸易战的紧张局势中,还存在一些误解。 <extra_id_59> 监视一下你是否仍在使用某种方式? <extra_id_59> 有一次,詹姆斯(Boo James)女士搭乘公共汽车,看到一名陌生女子向她招手。当时,她没有多想,事后才知道其实那是她的亲妈。 <extra_id_59> 2016年9月, 哈里王子和妻子梅根相继辞职,离开香港,开始了一年之久的访问。 <extra_id_59> 在美国,很多留学生抱怨说希望自己的孩子丢到考场。 <extra_id_59> 家常便饭的老师,对学校的反应不一。 <extra_id_59> 我相信你肯定同意我的这个看法:人们通常会认为你觉得自己有多聪明,不是吗? <extra_id_59> 你是否压力过大,不堪重负,任务繁多? <extra_id_59> 经过了大约两年的调查和研究,我们决定把一本书的外形提交给《哈利·波特》。书中描述了这本书的一些经过改编而成的回忆录,希望能够帮助你获得高达15万英镑的收益。 <extra_id_59> 新冠病毒疫情让很多国家陷入凋零。 <extra_id_59> 毛泽东和陈水扁接受了特朗普总统的电话通话。 <extra_id_59> BBC中文网采访了一位访问过一个视频博主的经过。 <extra_id_59> 在美国访问的美国总统奥巴马,对布鲁克林的一个重要问题表示了支持。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 去除多余的空白和换行符\n",
    "WHITESPACE_HANDLER = lambda k: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', k.strip()))\n",
    "\n",
    "# 从文件读取无标点的输入文本\n",
    "with open(\"D:/output - Copy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    article_text = file.read()\n",
    "    \n",
    "# 加载 mT5 模型和分词器\n",
    "model_name = \"csebuetnlp/mT5_m2o_chinese_simplified_crossSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 按关键词分割文本成较短的句子\n",
    "split_text = re.sub(r\"(那么|然后|首先|所以|因此|之后|啊|嗯|我觉得|我想|我认为)\", r\". \\1\", article_text)\n",
    "split_sentences = split_text.split(\". \")  # 分割成短句子\n",
    "print(\"分割后的句子:\", split_sentences)\n",
    "\n",
    "# 对每个短句生成摘要\n",
    "summary_chunks = []\n",
    "for sentence in split_sentences:\n",
    "    cleaned_sentence = WHITESPACE_HANDLER(sentence)\n",
    "    if cleaned_sentence:  # 忽略空白句子\n",
    "        input_ids = tokenizer(\n",
    "            [cleaned_sentence],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # 生成摘要\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,  # 控制每段摘要的最大长度\n",
    "            no_repeat_ngram_size=2,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )[0]\n",
    "\n",
    "        # 解码生成的摘要\n",
    "        summary_chunk = tokenizer.decode(\n",
    "            output_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        summary_chunks.append(summary_chunk)\n",
    "\n",
    "# 合并所有段落摘要为最终摘要\n",
    "final_summary = \" \".join(summary_chunks)\n",
    "print(\"生成的最终摘要:\\n\", final_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "025dac02-bd78-4651-bfb2-b62d88ffe792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.7.0-cp311-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: sniffio in d:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\users\\user\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in d:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: colorama in d:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.7.0-cp311-none-win_amd64.whl (205 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: jiter, h11, distro, httpcore, httpx, openai\n",
      "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.7.0 openai-1.54.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5367fe-bb5c-469a-be53-8caa1e68e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割后的句子: ['', '啊好把这一部分的内容总结一下', '首先呢是一个人他有一个认知体系', '那么这个人认知体系他肯定会基于这个人的特色', '然后有一些特定的那个触发点', '那么这个出发点来自于这个人跟其他的事物进行一个交互那这个比如说他可能跟另一个人进行一段对话或者是看了一部电影看了一本书甚至死是听了一部而听了一个演讲', '然后他都会产生一些兴趣点或者是触发点但他觉得眼前一亮想要探索更多', '那么这些出发点之间的有一些可能是抢连结的有一些可能是弱连接的但是现在可能就在一段时间', '之后就就他就回忆不起来了但是强连接的呢他因为大脑比如说基于这个出发点他有很多在接受到这个出发点的时候他有很多联想比如说当然他是主动的进行联想或者是被动了一下', '然后呢这样的话就会让这个出发点呢就会牢牢的就是说被保留在了他的知识体系列在他的知识体系在这个范围内就不断的回扩大', '然后呢因为就是因为这个强连接', '那么这个新的知识听了这个出发点也变成了他只是体系的一个部分', '然后呢也在未来的使用当中更容易被调去但是但是那些弱点可能未来在未来的使用中就被遗忘就不会被调去了', '那么', '然后还有一个方法它能帮助我们去就是主动的去增强这些弱连结', '那么就是人为的进行主动的去教别人或者是去输出比如说路上一个视频写一段话当然在这个过程中呢他就会不断的主动的去掉去', '那么就会加深这个内容跟原有之体系的一些交互', '然后进行连接当然这部分如果要持续的话他就需要一个反馈那个反馈可能就是来自于比如说读者听众任何一个外部的环境', '那么这里就好像有一次的一些现象就是说我们在接收到好的反馈的时候我们的情感恼他会突然就会被激发他就会很快', '然后就是本人的一部分', '那么这个人做这个事情就非常有个动力我们想想一下你还来原来在在学校里面', '嗯跟就收到老师的表扬之类的或者是家长的这个认可', '那么就会特别有动力去把这个事情哦再见这里要干好但是呢如果遇到了负面的反馈的话那就要启动我们的理性脑那我们通过一种合理的解释不要掉入到这个方面富贵的这个循环里面这个负面的评价里面因为如果到了父母评价也', '那么人如果任由这个情绪控制的话那个情绪就会被外界影响就会也变得负面但其实这期的并不重要因为总会有一部分人他会因为我们的一些内容出出', '然后呢得到一些收益但是我们签订要讨论的是我们是不是这一部分内容它是经过我们精心打磨就是清新打造的就像书中描述的一样他说', '我想把我们的作品当成一个孩子一样带出去打扮的漂漂亮亮', '我觉得这个解释平衡的形象生动也让', '我觉得我对于做这个视频博主的这个事情有了一些深入的一些思考和理解', '我觉得这可以是一个很好的一个方向']\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "The model `gpt-4` does not exist or you do not have access to it.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m split_sentences:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39mstrip():  \u001b[38;5;66;03m# 忽略空白句子\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m         response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     21\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m             messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     23\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m请生成一个简短的中文摘要。\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     24\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentence}\n\u001b[0;32m     25\u001b[0m             ],\n\u001b[0;32m     26\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[0;32m     27\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     28\u001b[0m         )\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m# 提取生成的摘要\u001b[39;00m\n\u001b[0;32m     31\u001b[0m         summary \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mD:\\Users\\User\\anaconda3\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32mD:\\Users\\User\\anaconda3\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mD:\\Users\\User\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mD:\\Users\\User\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    702\u001b[0m             result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Users\\User\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: The model `gpt-4` does not exist or you do not have access to it."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import openai\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "openai.api_key = \"sk----\"\n",
    "\n",
    "# 从文件读取无标点的输入文本\n",
    "with open(\"D:/output - Copy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    article_text = file.read()\n",
    "    \n",
    "# 按关键词分割文本成较短的句子\n",
    "split_text = re.sub(r\"(那么|然后|首先|所以|因此|之后|啊|嗯|我觉得|我想|我认为)\", r\". \\1\", article_text)\n",
    "split_sentences = split_text.split(\". \")  # 分割成短句子\n",
    "print(\"分割后的句子:\", split_sentences)\n",
    "\n",
    "# 对每个短句子生成摘要\n",
    "summary_chunks = []\n",
    "for sentence in split_sentences:\n",
    "    if sentence.strip():  # 忽略空白句子\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"请生成一个简短的中文摘要。\"},\n",
    "                {\"role\": \"user\", \"content\": sentence}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        # 提取生成的摘要\n",
    "        summary = response['choices'][0]['message']['content']\n",
    "        summary_chunks.append(summary)\n",
    "\n",
    "# 合并所有句子的摘要\n",
    "final_summary = \" \".join(summary_chunks)\n",
    "print(\"生成的最终摘要:\\n\", final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668c83f6-eab0-4622-8899-ee6f5f588676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割后的句子: ['', '啊好把这一部分的内容总结一下', '首先呢是一个人他有一个认知体系', '那么这个人认知体系他肯定会基于这个人的特色', '然后有一些特定的那个触发点', '那么这个出发点来自于这个人跟其他的事物进行一个交互那这个比如说他可能跟另一个人进行一段对话或者是看了一部电影看了一本书甚至死是听了一部而听了一个演讲', '然后他都会产生一些兴趣点或者是触发点但他觉得眼前一亮想要探索更多', '那么这些出发点之间的有一些可能是抢连结的有一些可能是弱连接的但是现在可能就在一段时间', '之后就就他就回忆不起来了但是强连接的呢他因为大脑比如说基于这个出发点他有很多在接受到这个出发点的时候他有很多联想比如说当然他是主动的进行联想或者是被动了一下', '然后呢这样的话就会让这个出发点呢就会牢牢的就是说被保留在了他的知识体系列在他的知识体系在这个范围内就不断的回扩大', '然后呢因为就是因为这个强连接', '那么这个新的知识听了这个出发点也变成了他只是体系的一个部分', '然后呢也在未来的使用当中更容易被调去但是但是那些弱点可能未来在未来的使用中就被遗忘就不会被调去了', '那么', '然后还有一个方法它能帮助我们去就是主动的去增强这些弱连结', '那么就是人为的进行主动的去教别人或者是去输出比如说路上一个视频写一段话当然在这个过程中呢他就会不断的主动的去掉去', '那么就会加深这个内容跟原有之体系的一些交互', '然后进行连接当然这部分如果要持续的话他就需要一个反馈那个反馈可能就是来自于比如说读者听众任何一个外部的环境', '那么这里就好像有一次的一些现象就是说我们在接收到好的反馈的时候我们的情感恼他会突然就会被激发他就会很快', '然后就是本人的一部分', '那么这个人做这个事情就非常有个动力我们想想一下你还来原来在在学校里面', '嗯跟就收到老师的表扬之类的或者是家长的这个认可', '那么就会特别有动力去把这个事情哦再见这里要干好但是呢如果遇到了负面的反馈的话那就要启动我们的理性脑那我们通过一种合理的解释不要掉入到这个方面富贵的这个循环里面这个负面的评价里面因为如果到了父母评价也', '那么人如果任由这个情绪控制的话那个情绪就会被外界影响就会也变得负面但其实这期的并不重要因为总会有一部分人他会因为我们的一些内容出出', '然后呢得到一些收益但是我们签订要讨论的是我们是不是这一部分内容它是经过我们精心打磨就是清新打造的就像书中描述的一样他说', '我想把我们的作品当成一个孩子一样带出去打扮的漂漂亮亮', '我觉得这个解释平衡的形象生动也让', '我觉得我对于做这个视频博主的这个事情有了一些深入的一些思考和理解', '我觉得这可以是一个很好的一个方向']\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "API 请求出错: The model `gpt-4` does not exist or you do not have access to it.\n",
      "生成的最终摘要:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "# 如果 API Key 已存储在环境变量中，可以直接使用 os.environ['key']\n",
    "openai.api_key = os.getenv(\"key\", \"--\")  # 替换 \"your_openai_api_key_here\" 为实际的 API Key\n",
    "\n",
    "# 定义空白符处理函数\n",
    "WHITESPACE_HANDLER = lambda k: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', k.strip()))\n",
    "\n",
    "# 从文件读取无标点的输入文本\n",
    "with open(\"D:/output - Copy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    article_text = file.read()\n",
    "    \n",
    "# 按关键词分割文本成较短的句子\n",
    "split_text = re.sub(r\"(那么|然后|首先|所以|因此|之后|啊|嗯|我觉得|我想|我认为)\", r\". \\1\", article_text)\n",
    "split_sentences = split_text.split(\". \")  # 分割成短句子\n",
    "print(\"分割后的句子:\", split_sentences)\n",
    "\n",
    "# 对每个短句子生成摘要\n",
    "summary_chunks = []\n",
    "for sentence in split_sentences:\n",
    "    cleaned_sentence = WHITESPACE_HANDLER(sentence)\n",
    "    if cleaned_sentence:  # 忽略空白句子\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"请生成一个简短的中文摘要。\"},\n",
    "                    {\"role\": \"user\", \"content\": cleaned_sentence}\n",
    "                ],\n",
    "                max_tokens=150,\n",
    "                temperature=0.5\n",
    "            )\n",
    "\n",
    "            # 提取生成的摘要\n",
    "            summary = response['choices'][0]['message']['content']\n",
    "            summary_chunks.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"API 请求出错: {e}\")\n",
    "            continue\n",
    "\n",
    "# 合并所有句子的摘要\n",
    "final_summary = \" \".join(summary_chunks)\n",
    "print(\"生成的最终摘要:\\n\", final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a21269-f8a3-4405-b4e0-5c6190914291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 从文件读取无标点的输入文本\n",
    "with open(\"D:/output - Copy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    article_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6042d67-4441-42bb-9049-54e06def48e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec625d-4dd3-4cfd-be27-6afb5ca6918e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f0f54-a9d3-4253-a51b-60db0eead72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7cd46c-855c-4fd5-99e2-289cbdcf94d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7bbc4-d1eb-4e1c-9b56-92511422f8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae5614-7e9e-49f2-ad46-abb9ed862220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e03b1c-6a67-4f25-a94e-5a2620d3fa09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f054eb5-885d-4a3f-a0c1-508817b5c44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac1fd847d744128bb4959f1de3f9645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--uer--t5-v1_1-base-chinese-cluecorpussmall. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac4a7b0ae374659a774f29ab3cd3a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1702525316054fc98f37249b4707bc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924e451866654d5daad5be08526ae7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef15dc5b1d74910bf6dd0fa49b87397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/923M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割后的句子: ['啊好把这一部分的内容总结一下', '首先呢是一个人他有一个认知体系', '那么这个人认知体系他肯定会基于这个人的特色', '然后有一些特定的那个触发点', '那么这个出发点来自于这个人跟其他的事物进行一个交互那这个比如说他可能跟另一个人进行一段对话或者是看了一部电影看了一本书甚至死是听了一部而听了一个演讲', '然后他都会产生一些兴趣点或者是触发点但他觉得眼前一亮想要探索更多', '那么这些出发点之间的有一些可能是抢连结的有一些可能是弱连接的但是现在可能就在一段时间', '之后就就他就回忆不起来了但是强连接的呢他因为大脑比如说基于这个出发点他有很多在接受到这个出发点的时候他有很多联想比如说当然他是主动的进行联想或者是被动了一下', '然后呢这样的话就会让这个出发点呢就会牢牢的就是说被保留在了他的知识体系列在他的知识体系在这个范围内就不断的回扩大', '然后呢因为就是因为这个强连接', '那么这个新的知识听了这个出发点也变成了他只是体系的一个部分', '然后呢也在未来的使用当中更容易被调去但是但是那些弱点可能未来在未来的使用中就被遗忘就不会被调去了', '那么', '然后还有一个方法它能帮助我们去就是主动的去增强这些弱连结', '那么就是人为的进行主动的去教别人或者是去输出比如说路上一个视频写一段话当然在这个过程中呢他就会不断的主动的去掉去', '那么就会加深这个内容跟原有之体系的一些交互', '然后进行连接当然这部分如果要持续的话他就需要一个反馈那个反馈可能就是来自于比如说读者听众任何一个外部的环境', '那么这里就好像有一次的一些现象就是说我们在接收到好的反馈的时候我们的情感恼他会突然就会被激发他就会很快', '然后就是本人的一部分', '那么这个人做这个事情就非常有个动力我们想想一下你还来原来在在学校里面嗯跟就收到老师的表扬之类的或者是家长的这个认可', '那么就会特别有动力去把这个事情哦再见这里要干好但是呢如果遇到了负面的反馈的话那就要启动我们的理性脑那我们通过一种合理的解释不要掉入到这个方面富贵的这个循环里面这个负面的评价里面因为如果到了父母评价也', '那么人如果任由这个情绪控制的话那个情绪就会被外界影响就会也变得负面但其实这期的并不重要因为总会有一部分人他会因为我们的一些内容出出', '然后呢得到一些收益但是我们签订要讨论的是我们是不是这一部分内容它是经过我们精心打磨就是清新打造的就像书中描述的一样他说我想把我们的作品当成一个孩子一样带出去打扮的漂漂亮亮我觉得这个解释平衡的形象生动也让我觉得我对于做这个视频博主的这个事情有了一些深入的一些思考和理解我觉得这可以是一个很好的一个方向']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(**inputs, max_length=100, num_beams=4, early_stopping=True)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m optimized_sentence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m optimized_sentences\u001b[38;5;241m.\u001b[39mappend(optimized_sentence)\n",
      "File \u001b[1;32mD:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1972\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1969\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[0;32m   1971\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_kwargs(model_kwargs\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1360\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[1;34m(self, model_kwargs)\u001b[0m\n\u001b[0;32m   1357\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[1;32m-> 1360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1361\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1363\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f958c3511ed49a3bb6ba4357fa04199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/923M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 加载Flan-T5 large模型和分词器\n",
    "model_name = \"uer/t5-v1_1-base-chinese-cluecorpussmall\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 从文件读取无标点的输入文本\n",
    "with open(\"D:/output - Copy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "# 自动分割文本成更短的句子\n",
    "# 假设用“那么”、“然后”等词作为自然的分隔点插入句号\n",
    "split_text = re.sub(r\"(那么|然后|首先|所以|因此|之后)\", r\". \\1\", input_text)\n",
    "split_sentences = split_text.split(\". \")  # 分割成短句子\n",
    "print(\"分割后的句子:\", split_sentences)\n",
    "\n",
    "# 对每个短句子生成优化后的文本\n",
    "optimized_sentences = []\n",
    "for sentence in split_sentences:\n",
    "    if sentence.strip():  # 忽略空白句子\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        # outputs = model.generate(**inputs, max_length=100, num_beams=4, early_stopping=True)\n",
    "        outputs = model.generate(**inputs, max_length=300, num_beams=6, early_stopping=True)\n",
    "\n",
    "        optimized_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        optimized_sentences.append(optimized_sentence)\n",
    "\n",
    "# 将优化后的文本写入到输出文件\n",
    "with open(\"optimized_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\". \".join(optimized_sentences) + \".\")\n",
    "\n",
    "# 打印优化后的文本\n",
    "optimized_text = \". \".join(optimized_sentences) + \".\"\n",
    "print(\"优化后的文本:\\n\", optimized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90853c6-24a2-4d56-9f41-2dbb7526c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 加载 T5 中文模型和分词器\n",
    "model_name = \"uer/t5-v1_1-base-chinese-cluecorpussmall\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 从文件读取无标点的输入文本\n",
    "with open(\"D:/output - Copy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "# 自动分割文本成更短的句子\n",
    "# 假设用“那么”、“然后”等词作为自然的分隔点插入句号\n",
    "split_text = re.sub(r\"(那么|然后|首先|所以|因此|之后)\", r\". \\1\", input_text)\n",
    "split_sentences = split_text.split(\". \")  # 分割成短句子\n",
    "print(\"分割后的句子:\", split_sentences)\n",
    "\n",
    "# 对每个短句子生成优化后的文本\n",
    "optimized_sentences = []\n",
    "for sentence in split_sentences:\n",
    "    if sentence.strip():  # 忽略空白句子\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        # 移除 token_type_ids\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "        # 生成优化后的文本\n",
    "        outputs = model.generate(**inputs, max_length=300, num_beams=6, early_stopping=True)\n",
    "        optimized_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        optimized_sentences.append(optimized_sentence)\n",
    "\n",
    "# 将优化后的文本写入到输出文件\n",
    "with open(\"optimized_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\". \".join(optimized_sentences) + \".\")\n",
    "\n",
    "# 打印优化后的文本\n",
    "optimized_text = \". \".join(optimized_sentences) + \".\"\n",
    "print(\"优化后的文本:\\n\", optimized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc898424-1ccc-4a6e-abf1-9ce7510b2601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4759a-e3c7-4998-97d7-1f7c5cbc375a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa329f-2b6f-4a45-b4c8-c30ba2a49bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e5411-6308-4764-a614-3c0f69915373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be828f25-d511-45af-b276-9070901ef153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e4391-3d38-4b51-a224-25d80505bdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc638c4-6ddd-4bbe-8462-35adb008d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38394e0-5413-437e-878b-ad3371a0c814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c228a47-e275-4773-b514-7b1afb414b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421399a-e4a9-445f-815b-1e69f7b0a5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78f6c1-9597-4150-baa1-89e6ddb0a660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b9d34-c776-4691-82f4-96781356c8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca40713-4db7-4087-b870-59c9a1b92291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: 读取保存的 .txt 文件内容\n",
    "with open(\"D:/output.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8add15c3-e53a-4db0-b44e-38b0216b05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小型模型（t5-small、flan-t5-small）：适合显存较小的 GPU（4GB 左右）或 CPU 运行。\n",
    "# 中型模型（t5-base、flan-t5-base）：建议使用 4-8GB 的显存，性能会有所提升。\n",
    "# 大型模型（t5-large、flan-t5-large）：需要 8GB 或更高的显存。\n",
    "# 超大模型（t5-3b、t5-11b）：通常需要 16GB 或更多的显存，或者使用分布式计算环境（如云 GPU）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d09478-fce8-4bbe-a9c5-c911294b1bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2103a4e0-db00-4faa-a255-7a37f1b72b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Step 4: 使用 T5 模型对文本进行润色\n",
    "model_name = \"t5-small\"  # 可以选择更大的模型，如 t5-large\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53db44f8-76e5-4ad5-90c9-28b2f3c3be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本格式为 T5 任务提示\n",
    "input_text = \"paraphrase: \" + text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de7eb78-d2f4-4b6b-80f6-137539d96a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "润色后的文本： paraphrase:\n"
     ]
    }
   ],
   "source": [
    "# 使用 T5 模型生成润色后的文本\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=512, num_beams=4, early_stopping=True)\n",
    "paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"润色后的文本：\", paraphrased_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de14d5b5-a49e-462e-ac31-a97618c7bf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "润色后的文本已保存到 D:/output_paraphrased.txt 文件中\n"
     ]
    }
   ],
   "source": [
    "# Step 5: 将润色后的文本保存到新的 .txt 文件\n",
    "with open(\"D:/output_paraphrased.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(paraphrased_text)\n",
    "print(\"润色后的文本已保存到 D:/output_paraphrased.txt 文件中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b0c94-d06f-4474-bc5d-0558a0ddcab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2598bd1-62dc-430f-ba08-6ca4f42581f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
